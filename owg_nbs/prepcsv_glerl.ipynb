{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e232f9a-6a7a-49cf-8caa-d45aa81e9111",
   "metadata": {},
   "source": [
    "# Prepare a CSV file for the OWG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746a17dc-456c-4269-9e2f-659b54b3a755",
   "metadata": {},
   "source": [
    "This notebook contains code as a function and a walk through for creating a csv file that can be use in optical wave gauging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1253f43-1469-4d30-b07d-d87511fdf51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages and libs\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import calendar\n",
    "from scipy import interpolate\n",
    "import os\n",
    "import shutil\n",
    "import csv\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30abc9df-d19d-4062-b93c-05d51502eca7",
   "metadata": {},
   "source": [
    "\n",
    "## Read data\n",
    "\n",
    "Data comes from two sources:  \n",
    "* 1. NDBC standard meteorlogical observation archives, found [here](https://www.ndbc.noaa.gov/) \n",
    "* 2. Archived ReCON imagery, partial records can be scraped from the [website](https://www.glerl.noaa.gov/metdata/) or accessed from GLERL's network\n",
    "\n",
    "\n",
    "This data must be joined so that each webcam image is associated with the meterological observations taken no later than 30 minutes before or after the image was captured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2fbeabe-17c5-461a-8ce3-8dbfdb8e130b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete csv\n",
    "\n",
    "def waveframetocsv(fn, target, csvfile, directory, localtimezone):\n",
    "    '''\n",
    "    This function takes a txt file and folder of OWG images and combines the data into a csv file that can be fed into the OWG. \n",
    "    version 2.0\n",
    "    \n",
    "    fn: filename and location of .txt file\n",
    "    target: attribute that the owg will be predicting\n",
    "    csvfile:the name of the csvfile being created\n",
    "    directory: the directory of images that have been prepped for OWG filtering\n",
    "    localtimezone: the timezone code in pytz that the images were taken in\n",
    "    '''\n",
    "    \n",
    "    #counters for debugging and perfomance info\n",
    "    initaladdcounter = 0\n",
    "    rangecounter = 0\n",
    "    successcounter = 0\n",
    "    failcounter = 0\n",
    "    timecounter = 0\n",
    "    outofrangecounter = 0\n",
    "    \n",
    "    # create a dataframe from the buoy data\n",
    "    df = pd.read_csv(fn, skiprows=range(1,2), delim_whitespace = True, \\\n",
    "                    parse_dates={'date':[0,1,2,3,4]}, keep_date_col=False)\n",
    "\n",
    "    # Transfer data in \"date\" column to a column where it is stored as a datetime object\n",
    "    df['datetime'] = pd.to_datetime(df['date'], format = '%Y %m %d %H %M',utc=True)\n",
    "    df = df.drop(df.columns[[0,1,2,3,6,8,9,10,11,12, 13]], axis = 1)\n",
    "    \n",
    "    # calculate unix datetime\n",
    "    df['epoch']=(df['datetime'] - pd.Timestamp(\"1970-01-01\",tz='utc')) // pd.Timedelta('1s')\n",
    "    \n",
    "    print(df)\n",
    "    \n",
    "    # delete the csv file if it exsists\n",
    "    try:\n",
    "        print (\"Overwriting csv file\")\n",
    "        os.remove(csvfile)\n",
    "        owgframe = pd.DataFrame({\"id\":[], \"H\":[], \"T\":[], \"MWDIR\":[]})\n",
    "    except:\n",
    "        print(\"couldn't find file, making new one\")\n",
    "        # create csv file that will be appended to by loop\n",
    "        owgframe = pd.DataFrame({\"id\":[], \"H\":[], \"T\":[], \"MWDIR\":[]})\n",
    "\n",
    "    #loop through directory and extract unix timestamp\n",
    "    for filename in os.listdir(directory):\n",
    "        # Use string slicing to remove .jpg from filename\n",
    "        size = len(filename)\n",
    "        fn = filename[:size - 4]\n",
    "       \n",
    "        \n",
    "        #get utc time from filename\n",
    "        local = pytz.timezone(localtimezone)\n",
    "        naive = datetime.datetime.strptime(fn, \"%Y%m%d%H%M\")\n",
    "        local_dt = local.localize(naive)\n",
    "        utc_dt = local_dt.astimezone(pytz.utc)\n",
    "        utime = calendar.timegm(utc_dt.timetuple())\n",
    "        \n",
    "        # combine datasets\n",
    "        try:\n",
    "            # track if buoy was in the water when the image was taken and find the closest record\n",
    "            if utime >= df['epoch'].iat[0]:\n",
    "                result_index = df['epoch'].sub(utime).abs().idxmin()\n",
    "                result = df['epoch'].iat[result_index]\n",
    "                rangecounter = rangecounter + 1\n",
    "                \n",
    "                # determine if the closest record was within 30 minutes and add to final dataframe\n",
    "                if abs(utime - result) <= 1800:\n",
    "                    initaladdcounter = initaladdcounter + 1\n",
    "                    stageframe = {\"id\":filename, \"H\": df['WVHT'].iat[result_index], \"T\": df['DPD'].iat[result_index], \"MWDIR\":df['MWD'].iat[result_index]}\n",
    "                    owgframe = owgframe.append(stageframe, ignore_index = True)\n",
    "                    \n",
    "                    # drop NAN values for the target variable\n",
    "                    if target == \"WVHT\":\n",
    "                        owgframe = owgframe[owgframe.H != 99.0]\n",
    "                    if target == \"DPD\":\n",
    "                        owgframe = owgframe[owgframe.DPD != 99.0]\n",
    "                    if target == \"MWD\":\n",
    "                        owgframe = owgframe[owgframe.MWDIR != 999]\n",
    "                else:\n",
    "                    timecounter = timecounter + 1\n",
    "            else:\n",
    "                outofrangecounter = outofrangecounter + 1\n",
    "        except:\n",
    "            failcounter = failcounter + 1\n",
    "    \n",
    "    # write final dataframe to csv\n",
    "    added = len(owgframe.index)\n",
    "    owgframe.to_csv(csvfile, index = False)    \n",
    "    \n",
    "    # display performance information\n",
    "    print (\"you have {} images during buoy deployment\".format(rangecounter))\n",
    "    print (\"{} images outside of buoy deployment\".format(outofrangecounter))\n",
    "    print (\"{} images do not have data within 30 minutes\".format(timecounter))\n",
    "    print (\"{} images had NAN data from buoy\".format(initaladdcounter-int(added)))\n",
    "    print (\"{} images failed\".format(failcounter))\n",
    "    print (\"{} images added to {}\".format(added, csvfile))\n",
    "    \n",
    "   \n",
    "    \n",
    "    return  \n",
    "                   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdbb8ae-5d65-47b3-8e45-1d5dfe2eb97b",
   "metadata": {},
   "source": [
    "\n",
    "### Run function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d091f535-449b-4b14-850e-ae6785a4e591",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       WVHT    DPD  MWD                  datetime       epoch\n",
      "0      0.41   2.68   24 2021-05-08 22:00:00+00:00  1620511200\n",
      "1      0.39   2.78   63 2021-05-08 22:10:00+00:00  1620511800\n",
      "2      0.39   2.45   65 2021-05-08 22:20:00+00:00  1620512400\n",
      "3      0.36   2.62   84 2021-05-08 22:30:00+00:00  1620513000\n",
      "4      0.34   2.73   86 2021-05-08 22:40:00+00:00  1620513600\n",
      "...     ...    ...  ...                       ...         ...\n",
      "20897  0.28   2.11  224 2021-10-14 11:30:00+00:00  1634211000\n",
      "20898  0.27   2.33  217 2021-10-14 11:40:00+00:00  1634211600\n",
      "20899  0.25   2.32  215 2021-10-14 11:50:00+00:00  1634212200\n",
      "20900  0.23   2.34  227 2021-10-14 12:10:00+00:00  1634213400\n",
      "20901  0.23  99.00  216 2021-10-14 12:20:00+00:00  1634214000\n",
      "\n",
      "[20902 rows x 5 columns]\n",
      "Overwriting csv file\n",
      "couldn't find file, making new one\n",
      "you have 1130 images during buoy deployment\n",
      "412 images outside of buoy deployment\n",
      "45 images do not have data within 30 minutes\n",
      "43 images had NAN data from buoy\n",
      "0 images failed\n",
      "1042 images added to C:/njc/src/GLERL_contract/buoy_data/mcy2021_prototype.csv\n"
     ]
    }
   ],
   "source": [
    "fn = \"C:/njc/src/GLERL_contract/buoy_data/mcy2021.txt\"\n",
    "target = \"WVHT\"\n",
    "csvfile = \"C:/njc/src/GLERL_contract/buoy_data/mcy2021_prototype.csv\"\n",
    "directory = \"D:/ReCON_imgs/mcy_total/2021\"\n",
    "waveframetocsv(fn, target, csvfile, directory, \"America/Chicago\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0b2709-21a1-4574-b839-5925a1ffb19d",
   "metadata": {},
   "source": [
    "This shows not only a huge increase in available data per trial, but also ensures much more accurate and higher quality data which can only better serve model performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b94825-1a8f-4bda-bd0c-5e9c16fb6d4b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Code for creating model assessment dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "319ee778-e0b1-4848-abd5-d1ec4a1932e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_ran(csv, sample_csv, sample_percent):\n",
    "    ''' This function selects for a percentage of random rows in the csv training dataset \n",
    "    and creates a new csv for model assessment while dropping them from the original csv\n",
    "    \n",
    "    csv : Path to the csv that the function initially reads\n",
    "    validation_csv : Path and name of the csv to be created from csv\n",
    "    sample_percent : decimal value of the percentage of rows to be sampled '''\n",
    "    \n",
    "    if os.path.exists(sample_csv) == True:\n",
    "        return print (\"File already exists\")\n",
    "    else:    \n",
    "        # open csv as a pandas dataframe \n",
    "        df = pd.read_csv(csv, sep = \",\")\n",
    "        # randomly sample data\n",
    "        df_sample = df.sample(n = int(sample_percent * len(df.index)))\n",
    "        # drop data from original dataframe that has been sampled\n",
    "        df = df[~df.id.isin(df_sample.id)]\n",
    "   \n",
    "        # export dataframes as csv files\n",
    "        df.to_csv(csv, sep = \",\", index = False) \n",
    "        df_sample.to_csv(validation_csv, sep = \",\", index = False)\n",
    "        print (\"writing {}\".format(sample_csv))\n",
    "    \n",
    "        return print (\"{}% of {} ({} rows) sampled\".format(sample_percent*100, csv, len(df_sample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7302b03c-f716-412e-9ec9-e98a30e72733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists\n"
     ]
    }
   ],
   "source": [
    "csv = \"C:/njc/src/GLERL_contract/buoy_data/mcy2021_prototype.csv\"\n",
    "sample_csv = \"C:/njc/src/GLERL_contract/buoy_data/mcy2021_prototype(sample).csv\"\n",
    "sample_percent = .3\n",
    "sample_ran(csv, sample_csv, sample_percent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
